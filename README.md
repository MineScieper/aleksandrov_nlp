# aleksandrov_nlp

## Установка

1. Клонируйте репозиторий:
```bash
git clone https://github.com/MineScieper/aleksandrov_nlp.git
cd aleksandrov_nlp
```
2. Создайте виртуальное окружение:
```bash
python -m venv my_venv
```
3. Активируйте виртуальное окружение:
```bash
# Linux/Mac
source my_venv/bin/activate

# Windows
my_venv\Scripts\activate
```
4. Установите зависимости:
```bash
pip install -r requirements.txt
```

## Структура
1. notebook.ipynb - jupyter notebook с кодом решения
2. word_frequency_flibusta_cleared_with_eng_words.txt.gz - gzip-архив со словарём русских слов, отсортированных по частоте использования (источник - https://github.com/Somewater/ruword_frequency/blob/master/data/word_frequency_flibusta_0.txt)
3. test_data.txt - набор данных, полученный из stepik
4. result.txt - результат, отправленный в stepik

## Метод решения - динамическое программирование + словарь
1. Использован pip-модуль wordninja_enhanced (https://github.com/timminator/wordninja-enhanced), являющийся модификацией модуля wordninja (https://github.com/keredson/wordninja), который поддерживает только английский язык.
2. Загружен и очищен словарь русских слов - слова, встречающиеся менее 100 раз были удалены из-за наличия в них склеенных слов (например, "курткакожаная"), которые мешают добавлению пробелов в предложения.
3. В конец словаря русских слов был добавлен словарь наиболее часто употребляемых английских слов (источник - https://github.com/zydou/high-frequency-words/blob/master/200k.txt) с целью определения английских слов (например, "Samsung", "iPhone")

## Достоинства и недостатки
### Достоинства
1. Быстродействие. 10000 предложений из 4 слов обрабатываются менее, чем за 3 секунды:
```python
import timeit

def func():
    return custom_lm.rejoin("предложениеизнесколькихслов")

print(timeit.timeit(func, number=10000))
# 2.8683789999922737
```
2. Экономия памяти. В решении нет тяжеловесных моделей-трансформеров, а датасет в сжатом виде занимает ~9 МБ памяти.
3. Обработка пунктуации. Предложенное решение оставляет знаки препинания на тех местах, где они были в исходном предложении.

### Недостатки
1. Некорректная обработка незнакомых слов. Слова, которых нет в словаре (например, имена и фамилии) не определяются как цельные слова и сегментируются по буквам ("Вова" - "Во в а", "Квентин" - "К вен тин", "ЕГЭ" - "ЕГ Э").

2. Некорректная обработка кавычек. По умолчанию пробел ставится после кавычки и не ставится до неё: Пришёл к колдунье:' А ну-ка наколдуй мне!'.


### Постскриптум
Мною были предприняты попытки дообучения моделей с Hugging Face (cointegrated/rut5-base, UrukHan/t5-russian-spell) на наборе русских предложений (https://github.com/Koziev/NLP_Datasets), из которых были убраны пробелы. К сожалению, добиться хоть сколько-нибудь вменяемых результатов так и не удалось.
